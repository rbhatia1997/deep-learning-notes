# Introduction
This will contain concepts and coding tools used for the first week of the tutorial. 

## Convolutional Model: Step by Step

We will be implementing the building blocks of a basic convolutional neural network. Because each forward function has a corresponding backwards equivalent, we store our parameters in a cache, which are used to compute gradients during back propagation. A convolution layer transforms an input volume into an output volume of a different size. 

Zero padding, the first thing we did, involves adding zeros around the border of an image. Padding allows you to use a CONV layer without shrinking the height/width of the volumes - useful for deeper networks. Also, you keep more info at the border of an image. We're using ```np.pad()```. 

Now, doing a single step of convolution. We convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up and adding a bias. In a forward pass, you take many filters and convolve them on the input. Each convolution gives you a 2D matrix output. To get a 2x2 slice at the upper left corner of a matrix, you do (a_slice_prev = a_prev[0:2,0:2,:]). 

The pooling layer reduces the height and width of the input; it reduces computation and makes feature detectors more invariant to the position in the input. 

## Convolutional Model: Application

The first step is creating placeholders for the input data that's fed into the model. None can be used for the batch size as that allows you to set it later. Weights and filters are taken care of in TensorFlow utilizing ```tf.contrib.layers.xavier_initializer(seed = 0)```. The biases are taken care of by TensorFlow. To initialize a parameter, you use the following: ```W = tf.get_variable("W", [1,2,3,4], initializer = ...```. In TensorFlow, there are a lot of functions that can do the convolution steps for you. ```tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME')``` : given an input  XX  and a group of filters  WW , this function convolves  WW 's filters on X. The third parameter ([1,s,s,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). Normally, you'll choose a stride of 1 for the number of examples (the first value) and for the channels (the fourth value), which is why we wrote the value as [1,s,s,1]. You can read the full documentation on conv2d. ```tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')```: given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. For max pooling, we usually operate on a single example at a time and a single channel at a time. So the first and fourth value in [1,f,f,1] are both 1. You can read the full documentation on max_pool. ```tf.nn.relu(Z)```: computes the elementwise ReLU of Z (which can be any shape). ```tf.contrib.layers.flatten(P)```: given a tensor "P", this function takes each training (or test) example in the batch and flattens it into a 1D vector. ```tf.contrib.layers.fully_connected(F, num_outputs)```: given the flattened input F, it returns the output computed using a fully connected layer.

Then, computing the cost function was the next step after building the model. The cost function helps the neural network see how much the predictions differ from the correct labels. ```tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y)``` : computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation softmax_cross_entropy_with_logits.
```tf.reduce_mean```: computes the mean of elements across dimensions of a tensor. Use this to calculate the sum of the losses over all the examples to get the overall cost. You can check the full documentation reduce_mean. So basically you use reduce mean (to get the overall cost, the sum of losses over all examples) on softmax to get cost. 

You can use tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) to create the optimizer. The optimizer has a minimize(loss=...) function that you'll call to set the cost function that the optimizer will minimize. If you took course 2 of the deep learning specialization, you implemented random_mini_batches() in the "Optimization" programming assignment. This function returns a list of mini-batches. It is already implemented in the cnn_utils.py file and imported here, so you can call it like this: minibatches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0). Within a loop, for each mini-batch, you'll use the tf.Session object (named sess) to feed a mini-batch of inputs and labels into the neural network and evaluate the tensors for the optimizer as well as the cost. Remember that we built a graph data structure and need to feed it inputs and labels and use sess.run() in order to get values for the optimizer and cost. You'll use this kind of syntax: ```output_for_var1, output_for_var2 = sess.run(fetches=[var1, var2], feed_dict={var_inputs: the_batch_of_inputs, var_labels: the_batch_of_labels}).```


