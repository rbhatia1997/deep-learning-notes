# Introduction

This Repository contains the notes that I have been writing for the Deep Learning Specialization. It'll cover my assignments and other notes so I can reference them after the course's completion. This is meant for my personal use, but I've made it public in case it's helpful to others. This is the fourth course.

## Week 1 

### Edge Detection 

Computer vision is an area of deep learning dedicated to detecting objects and is used in various applications. It's a highly innovative field. Computer vision has problems like image classification and object detection. Neural style transfer is transferring style to another image. Computer vision inputs can get really big; larger images make parameters pretty infeasible. 

Detecting vertical and horizontal edges can be a first step; you can construct a 3x3 matrix or filter or kernel. You take a 6x6 image and convolve it with that matrix; the output is a 4x4 matrix. You take the element wise product and addition of the 3x3 matrix (shifting) on the 6x6 matrix. This ends up being a vertical edge dectector; conv-forward in Python and tf.nn.conv2d is in TensorFlow. The middle of the image shows that there's a light to dark transition; if you flip the order from light to dark to dark to light, you notice the magnitude of the filtered output stays the same but has a negative middle ground. A horizontal filter is where it's bright on the top but dark on the bottom. Sobel filter puts more weight in the central row/pixel (more robust); sometimes, other filters are used (Scharr filter). You can learn the numbers (via parameters) to make an edge dectector. Backprop can actually learn the filter rather than using a hand-coded filter. 

### Types of Convolution

In convolution, you get a (n-f + 1) x (n-f +1) matrix. Pixels on the corners of the image in convolution aren't counted as much; throwing away a lot of information. Before convoluting, you can pad the image with a border of 1px (8x8) matrix and you get a (6x6) output. You can pad with two pixels if you want. In terms of how much to pad, valid and same convolutions; valid convolutions mean no padding. The other common choice is the same (pad so the output size is the same as the input size) -> (n + 2p - f + 1) x (n + 2p - f + 1) dimension. This implies that p is equal to (f - 1 / 2). Common for the filters to be odd because if you had it even, you'd have asymmetric padding; odd filters has a central position. 

Strided convolutions involve strides. Element-wise products as usual but you pop the box by two steps instead of just one step. The input/output are governed by the following: (n + 2p - f / stride) + 1 x (n + 2p - f / stride) + 1. You take the floor of the above in cases it doesn't work. Math textbooks usually has you flipping the filter matrix vertically and horizontally. This flipped matrix is what you then convolve. We don't really convolve, we are doing cross-correlation to be really specific. 

RGB images have height, width, and channels (channels match channels in the filter). Usually it's 3 for the channel because RGB. You multiply the filter by the various numbers over the various channels similar to that in the 2D case. If you want to get edges in the red channel, you can have the green/blue channels be all zeros. You can have a filter in all three channels to detect edges in all three channels. Convolving a volume gives a 4x4, 2D output. What about multiple filters at the same time? You can stack two filters together and output a volume. 

### Building Neural Network

You add a bias and apply a non-linearity to the filters. Then you stack the filters. This is one layer of the neural network. The filter multiplied by the image is similar to doing w*a; adding the real number is similar to getting to z. Activation is like adding the non-linearity term. If you had 10 filters, you'd only have 280 parameters. You can apply things to large images. If layer 1 is a convolutional layer, f[l], layer size, p[l] is padding, s[l] is the stride. The input to the layer is some nxnxn_c (channels); the output is just n x n x nc -> (n + 2p - f / s) + 1. a[l] will be nhl x nwl x ncl; each filter is just fl x fl x nc(l-1). A[l] is m x nhl x hwl x ncl. 

Let's say we're doing an image recognition task. Let's say you run your image through two filters then flatten the output and then give it to a softmax or logistic regression filter. Height and width gradually trend downwards. The number of channels generally increases. Usually there's three types of layers: convolution layer, pooling layer, and a fully conected layer. 

Pooling Layers: Max pooling. Split your 4x4 matrix into 2x2 sections and then take the max of each 2x2 region. Technically this is taking a stride of 2 and the f is 2. What this is doing is identifying specific features -> preserves the output. If this feature is detected anywhere, keep the number high. A lot of people use it; it has no parameters to learn. It's a fixed computation. There's also average pooling which takes the average of each filter size, but it's less commonly utilized. F is the filter size. 

Fully connected layer is like a single neural network layer. You connect each unit of the output to each unit of the fully connected layer. You have a standard weight matrix of a certain unit length connected to the output. Then you can attach a softmax function to output multiple classes. See what hyperparameters have worked in literature. A common pattern is having conv + pool layers + conv + pool layers + fc + fc layers + softmax. You go from an activation size of 3k to something much smaller. Max pooling layers have no paramters; activation size goes down gradually. 

Conv nets have small parameters because of parameter sharing (feature detector that's useful in one part of the image is probably useful in another part) and sparcity of connections (in each layer, each output value depends on a small number of input). Translation invariance - shifting images a few pixels - is well covered (due to the fact that the model is pretty robust). 

## Week 2

### Case Studies 

LeNet-5 was to recognize handwritten digits. They had two filters, two average pools, two fully connected layers, and a final output for predictions. Channels increase but the height/width go down; you may have conv layers followed by pooled layers followed by the fully connected layers. AlexNet uses a filter then max pooling then fully connected layers; similar to LeNet but much bigger. It used ReLU rather than tanh which assisted with its speed. The VGG (VGG-16) used a much simpler network; it simplified architecture. Pooling layers reduce height and width; doubling through each layer was good. Main downside was the number of parameters. 

Resnets use skip connections to let you train very deep connections. A residual block involves a linear block (multiply linear matrix and add bias). You then add a ReLU nonlinearity. Then you add the linear step again. Then you apply a ReLU equation again. When you take A[l] and push it deeper later in the neural network (shortcut/skip connection), you can use residual blocks to train deeper networks. You add skip connections (every two layers of a plain network is given an additional change and added together) to build a residual network. As you increase layers, your plain network may have worse error; however, Resnet has a decrease in training error despite being super deep - something that differentiates it from plain networks. 

If you make a network deeper, it can sometimes hurt your ability to do well on the training set. Not so much for ResNets. So basically, A[l+2] = g(W[l+2]*a[l+1] + b[l+1] + a[l]). The identify function is easy for a residual block to learn; skip connection makes it easy for a[l] to equal a[l+2]. In deep plain nets, when you make the network deeper and deeper, it's hard to learn the identity function and that hurts the performance. There'll be a lot of same convolutions because it lets you carry out addition of two equal vectors in Resnets. 

1x1 convolution involves you multiplying the image by a number; if you have a 6x6x32 thing that you convolve by a 1x1x32 convolution and then apply a non-linearity to it. You basically get a fully connected neural network that applies to each of the positions and end up with an output that is number of filters. Pooling layer lets you shrink the height and width. However, if you want to shrink/increase the channels, you can use 1x1 convolution. You can save on computations later. These convolutions lets you add non-linearity. Inception layer involves using a 1x1 convolution, max pooling, and same filters along with padding and then combining the outputs. Instead of picking one of the filter sizes/pooling stuff, you do them all and let the machine choose. The computational cost is the downside. 1x1 convolutions can be utilized to diminish this computational cost by shrinking a huge volume to an intermediate volume (called bottleneck layer). 

Inception network is just a bunch of inception blocks put together. Side branches takes some hidden layers to make a prediction; has a softmax to try to predict a label. It helps ensure that features are computed even in the hidden layers. 

### Practical Advice for using Convolutional Networks 

Hard to replicate these neural networks; a lot of researchers open source their work on Github. Look online for open source implementation. Pick an architecture you like and look for an open-source implementation and download from there! Downloading weights via pre-training is usually a lot easier than trying to figure it out on your own. So basically what transfer learning is doing is it's taking the softmax layer out and replacing it with your own that outputs to different classes. You freeze parameters in the other layers. By using someone else's pretrained weights, you can do this and build pretty effective models. Training shallow model after precomputing the previous frozen layers -> don't need to recompute the activation layers. If you have a larger dataset, you can freeze fewer layers and train later layers. If you have a lot of data, you can use the whole thing as initialization and train the whole thing. 