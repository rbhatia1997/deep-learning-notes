# Introduction

This Repository contains the notes that I have been writing for the Deep Learning Specialization. It'll cover my assignments and other notes so I can reference them after the course's completion. This is meant for my personal use, but I've made it public in case it's helpful to others. This is the fourth course.

## Week 1 

### Edge Detection 

Computer vision is an area of deep learning dedicated to detecting objects and is used in various applications. It's a highly innovative field. Computer vision has problems like image classification and object detection. Neural style transfer is transferring style to another image. Computer vision inputs can get really big; larger images make parameters pretty infeasible. 

Detecting vertical and horizontal edges can be a first step; you can construct a 3x3 matrix or filter or kernel. You take a 6x6 image and convolve it with that matrix; the output is a 4x4 matrix. You take the element wise product and addition of the 3x3 matrix (shifting) on the 6x6 matrix. This ends up being a vertical edge dectector; conv-forward in Python and tf.nn.conv2d is in TensorFlow. The middle of the image shows that there's a light to dark transition; if you flip the order from light to dark to dark to light, you notice the magnitude of the filtered output stays the same but has a negative middle ground. A horizontal filter is where it's bright on the top but dark on the bottom. Sobel filter puts more weight in the central row/pixel (more robust); sometimes, other filters are used (Scharr filter). You can learn the numbers (via parameters) to make an edge dectector. Backprop can actually learn the filter rather than using a hand-coded filter. 

### Types of Convolution

In convolution, you get a (n-f + 1) x (n-f +1) matrix. Pixels on the corners of the image in convolution aren't counted as much; throwing away a lot of information. Before convoluting, you can pad the image with a border of 1px (8x8) matrix and you get a (6x6) output. You can pad with two pixels if you want. In terms of how much to pad, valid and same convolutions; valid convolutions mean no padding. The other common choice is the same (pad so the output size is the same as the input size) -> (n + 2p - f + 1) x (n + 2p - f + 1) dimension. This implies that p is equal to (f - 1 / 2). Common for the filters to be odd because if you had it even, you'd have asymmetric padding; odd filters has a central position. 

Strided convolutions involve strides. Element-wise products as usual but you pop the box by two steps instead of just one step. The input/output are governed by the following: (n + 2p - f / stride) + 1 x (n + 2p - f / stride) + 1. You take the floor of the above in cases it doesn't work. Math textbooks usually has you flipping the filter matrix vertically and horizontally. This flipped matrix is what you then convolve. We don't really convolve, we are doing cross-correlation to be really specific. 

RGB images have height, width, and channels (channels match channels in the filter). Usually it's 3 for the channel because RGB. You multiply the filter by the various numbers over the various channels similar to that in the 2D case. If you want to get edges in the red channel, you can have the green/blue channels be all zeros. You can have a filter in all three channels to detect edges in all three channels. Convolving a volume gives a 4x4, 2D output. What about multiple filters at the same time? You can stack two filters together and output a volume. 

### Building Neural Network

You add a bias and apply a non-linearity to the filters. Then you stack the filters. This is one layer of the neural network. The filter multiplied by the image is similar to doing w*a; adding the real number is similar to getting to z. Activation is like adding the non-linearity term. If you had 10 filters, you'd only have 280 parameters. You can apply things to large images. If layer 1 is a convolutional layer, f[l], layer size, p[l] is padding, s[l] is the stride. The input to the layer is some nxnxn_c (channels); the output is just n x n x nc -> (n + 2p - f / s) + 1. a[l] will be nhl x nwl x ncl; each filter is just fl x fl x nc(l-1). A[l] is m x nhl x hwl x ncl. 

Let's say we're doing an image recognition task. Let's say you run your image through two filters then flatten the output and then give it to a softmax or logistic regression filter. Height and width gradually trend downwards. The number of channels generally increases. Usually there's three types of layers: convolution layer, pooling layer, and a fully conected layer. 

Pooling Layers: Max pooling. Split your 4x4 matrix into 2x2 sections and then take the max of each 2x2 region. Technically this is taking a stride of 2 and the f is 2. What this is doing is identifying specific features -> preserves the output. If this feature is detected anywhere, keep the number high. A lot of people use it; it has no parameters to learn. It's a fixed computation. There's also average pooling which takes the average of each filter size, but it's less commonly utilized. F is the filter size. 

Fully connected layer is like a single neural network layer. You connect each unit of the output to each unit of the fully connected layer. You have a standard weight matrix of a certain unit length connected to the output. Then you can attach a softmax function to output multiple classes. See what hyperparameters have worked in literature. A common pattern is having conv + pool layers + conv + pool layers + fc + fc layers + softmax. You go from an activation size of 3k to something much smaller. Max pooling layers have no paramters; activation size goes down gradually. 

Conv nets have small parameters because of parameter sharing (feature detector that's useful in one part of the image is probably useful in another part) and sparcity of connections (in each layer, each output value depends on a small number of input). Translation invariance - shifting images a few pixels - is well covered (due to the fact that the model is pretty robust). 

## Week 2

### Case Studies 

LeNet-5 was to recognize handwritten digits. They had two filters, two average pools, two fully connected layers, and a final output for predictions. Channels increase but the height/width go down; you may have conv layers followed by pooled layers followed by the fully connected layers. AlexNet uses a filter then max pooling then fully connected layers; similar to LeNet but much bigger. It used ReLU rather than tanh which assisted with its speed. The VGG (VGG-16) used a much simpler network; it simplified architecture. Pooling layers reduce height and width; doubling through each layer was good. Main downside was the number of parameters. 

Resnets use skip connections to let you train very deep connections. A residual block involves a linear block (multiply linear matrix and add bias). You then add a ReLU nonlinearity. Then you add the linear step again. Then you apply a ReLU equation again. When you take A[l] and push it deeper later in the neural network (shortcut/skip connection), you can use residual blocks to train deeper networks. You add skip connections (every two layers of a plain network is given an additional change and added together) to build a residual network. As you increase layers, your plain network may have worse error; however, Resnet has a decrease in training error despite being super deep - something that differentiates it from plain networks. 

If you make a network deeper, it can sometimes hurt your ability to do well on the training set. Not so much for ResNets. So basically, A[l+2] = g(W[l+2]*a[l+1] + b[l+1] + a[l]). The identify function is easy for a residual block to learn; skip connection makes it easy for a[l] to equal a[l+2]. In deep plain nets, when you make the network deeper and deeper, it's hard to learn the identity function and that hurts the performance. There'll be a lot of same convolutions because it lets you carry out addition of two equal vectors in Resnets. 

1x1 convolution involves you multiplying the image by a number; if you have a 6x6x32 thing that you convolve by a 1x1x32 convolution and then apply a non-linearity to it. You basically get a fully connected neural network that applies to each of the positions and end up with an output that is number of filters. Pooling layer lets you shrink the height and width. However, if you want to shrink/increase the channels, you can use 1x1 convolution. You can save on computations later. These convolutions lets you add non-linearity. Inception layer involves using a 1x1 convolution, max pooling, and same filters along with padding and then combining the outputs. Instead of picking one of the filter sizes/pooling stuff, you do them all and let the machine choose. The computational cost is the downside. 1x1 convolutions can be utilized to diminish this computational cost by shrinking a huge volume to an intermediate volume (called bottleneck layer). 

Inception network is just a bunch of inception blocks put together. Side branches takes some hidden layers to make a prediction; has a softmax to try to predict a label. It helps ensure that features are computed even in the hidden layers. 

### Practical Advice for using Convolutional Networks 

Hard to replicate these neural networks; a lot of researchers open source their work on Github. Look online for open source implementation. Pick an architecture you like and look for an open-source implementation and download from there! Downloading weights via pre-training is usually a lot easier than trying to figure it out on your own. So basically what transfer learning is doing is it's taking the softmax layer out and replacing it with your own that outputs to different classes. You freeze parameters in the other layers. By using someone else's pretrained weights, you can do this and build pretty effective models. Training shallow model after precomputing the previous frozen layers -> don't need to recompute the activation layers. If you have a larger dataset, you can freeze fewer layers and train later layers. If you have a lot of data, you can use the whole thing as initialization and train the whole thing. Transfer learning is usually the go-to unless you have an exceptionally large dataset. 

Data augmentation is usually utilized to improve performance. Mirroring on the vertical axis is one example (flipping an image of a cat). There's also random cropping (cropping to get various photos of something) -> it's not perfect, but works well. You could use shearing, rotation, local warping, etc. There's also color shifting (you can add to the RGB channels). PCA (PCA color augmentation) is basically running pre-processing (adding/subtracting colors to make it more standard). You can also implement distortions during training; for large datasets, you have a CPU thread that is streaming images but also implementing distortions and the data is passed to another thread or process. A common way to implement data augmentation is passing things to a different process/running things in parallel. There are hyperparamters in this thing as well. 

Simpler problems exist in areas where there's more data; there's more hacks/hand-generating when doing things with less data. There's labeled data and hand-engineering that are sources of knowledge. Transfer learning is a great thing to solve cases where's there relatively little data. People are really into doing well on standardized/benchmarks. 

## Week 3

### Detection Algorithms 

Localization means locating where the item detected is. This is called classification with localization. Object detection is when there is multiple objects in the picture. Classification with localization involves a ConvNet with a Softmax. If you want to localize the car in the image as well, you change the neural network to output a bounding box (bx,by,bh, and bw) -> where this the rectange that identifies items in an image. Bx and By is the center where bw and bh is height and width assuming the top left is (0,0). These are all percent of the size of the image. Y is a vector that indicates whether there is an object (pc is the probability that a class you're trying to detect is there). Then, bx, by, bh, bw is output; and c1, c2, and c3 (the name of the classes) is output. You can put ? if there are no objects. The loss function is the squared error but does depend on pc. 

In more general cases, you can just detect landmarks instead of doing the bounding box (just doing the x,y coordinates). You could define some landmarks of what you're detecting and generating a labeled training set of features; you could use a ConvNet that outputs stuff. You need a labeled training set where you annotate all the landmarks. The identity of landmark 1 must be consistent -> must be consistent. 

If you want to detect cars, you create a data training set where you have closely cropped data set of cars. You can train a ConvNet that trains images that detects cars; you can use this in sliding windows detection then. You would input a small rectangular region in the ConvNet. Then you keep passing the shifted window region to the ConvNet. You can then repeat this task with a larger window. You can do that with even larger windows -> there'll be a window where you can detect a car. The huge disadvantage to this is the computational cost. If you use a big stride, you reduce granularity. With ConvNets, this method is really slow. 

### Convolutional Implementations 

We can implement the convolutional implementation of the sliding windows detection method. Basically, by utilizing convolutions and filters, you can do this. Your neural network, let's say, outputs a 1x1x4 output. Convolutional neural networks basically have a lot of duplicate steps so if you cleverly run a filter, max pool, and three FC's, you can actually get the output of four propagations at once instead of plugging in four seperate input windows. You can implement the entire image and then have the predictions work - it's much more efficient. The bounding box position, then, however, isn't too accurate with this method. 

YOLO algorithm (you only look once). You place a grid on the image. Basic idea is you take the image classification algorithm and apply it to each of the nine grid cells. For each grid cell, you define a eight dimensional vector with bx,by,bh,bw, the classes, and if a class was detected. YOLO takes midpoint of the objects and assigns the object to the gridcell containing the object. Target output is 3x3x8 because you have a 8 dimensonal output and three grid cells with detected output.Then, when you do training, you can find the bounding box and note whether or not there's actually an item there. Each object only is assigned to one grid cell by midpoint assignment. YOLO does real time object detection and it's pretty face. Encoding the bounding box involves noting the fraction of the overall width and height as well as position from the top right/left corners. 

Intersection over union is what's used to evaluate your object detection algorithm. This function (IOU) computes the intersection over the union -> size of intersection and divides it by the size of the union (both bounding boxes size). Correct is usually if the IOU is greater than or equal than 0.5. It's usually a measure of the overlap/similarity. 

Non-max suppression is a way to ensure that an object is always detected only once. If you want to detect pedestrians, you may place a grid over the image. It's possible that certain grid cells may output midpoint of object's bounding boxes; non max suppression examples will say prevent all grid cells detect the same object. Non max suppression cleans up these detections by looks at the probabilities of detection; it takes the largest one and highlights that. It looks at the remaining bounding boxes with high overlap and then suppress those outputs. The rectange with the highest probability are not suppressed. Basically discard all boxes with PC less than 0.6 and output box with the largest PC. You independently output non max suppression for each class. 

Each grid cell can only detect one object; that's not great, so we use an idea called anchor boxes. If Y outputs a vector, it won't be able to output two ideas. Anchor boxes lets you have many items in one grid cell. You define outputs and associate them to the seperate anchor boxes. In other words, instead of having each object assigned to the grid cell via midpoint, you have each object assigned to each grid cell and anchor box with the highest IOU -> gets assigned to grid cell and anchor box cell. Output dimension changes based on number of anchor boxes.

### YOLO Algorithm 

You go through the grid cells and then get some output vector. The target Y changes depending on what the PC is. You train a ConvNet that inputs an image and outputs an output volume. Then you run everything through non-max suppression. One of the best algorithms out there. 

Region proposal (R-CNN) -> downside of YOLO is that it classifies stuff that may not be present (no objects). The idea is that R-CNN or regions with convolutional networks means you select just a few windows -> they run a segmentation algorithm to note which regions to run a classifier on. This R-CNN algorithm is still slow; it also outputs a bounding box and gets a more accurate bounding box. Downside is that it was slow; the improvements are Fast R-CNN (using convolutional sliding windows) and speeds up R-CNN. Fast R-CNN (clustering is still slow) made a Faster R-CNN algorithm and uses a convolutional network to propose regions (still slower than YOLO). 

## Week 4